from pyflink.common import Types

TEST_ARS_WORKFLOW_SCHEMA = {
    "workflow_id": Types.STRING(),
    "workflow_type": Types.STRING(),
    "workflow_name": Types.STRING(),
    "category": Types.STRING(),
    "device": Types.STRING(),
    "device_num": Types.INT(),
    "user": Types.STRING(),
    "data_source": Types.STRING(),
    "upload_ttl": Types.FLOAT(),
    "bag_nums": Types.INT(),
    "workflow_input": Types.STRING(),
    'workflow_output': Types.STRING(),
    'log': Types.STRING(),
    'workflow_status': Types.STRING(),
    'priority': Types.INT(),
    'tag': Types.STRING(),
    'hook': Types.STRING(),
    'create_time': Types.STRING(),
    'update_time': Types.STRING(),
    'batch_id_id': Types.STRING(),
    'tos_id': Types.STRING(),
    'metric': Types.STRING(),
}

TEST_ARS_BAG_SCHEMA = {
    'result_id': Types.STRING(),
    'pod_id': Types.STRING(),
    'job_id': Types.STRING(),
    'type': Types.STRING(),
    'node_name': Types.STRING(),
    'cluster_name': Types.STRING(),
    'status': Types.STRING(),
    'pod_start_timestamp': Types.FLOAT(),
    'playback_start_timestamp': Types.FLOAT(),
    'playback_end_timestamp': Types.FLOAT(),
    'device': Types.STRING(),
    'device_num': Types.INT(),
    'priority': Types.INT(),
    'group': Types.STRING(),
    'error_type': Types.STRING(),
    'error_details': Types.STRING(),
    'error_stage': Types.STRING(),
    'log': Types.STRING(),
    'data_source': Types.STRING(),
    'input_bag': Types.STRING(),
    'output_bag': Types.STRING(),
    'metric': Types.STRING(),
    'release': Types.STRING(),
    'coredump': Types.STRING(),
    'backtrace': Types.STRING(),
    'final_attempt': Types.BOOLEAN(),
    'config': Types.STRING(),
}

POD_ERR_SCHEMA = {
    "node_name": Types.STRING(),
    "job_status": Types.STRING(),
    "job_name": Types.STRING(),
    "timestamp": Types.STRING(),
}
